from collections import defaultdict

from surprise import Dataset
from surprise import SVD
from surprise import Reader
from surprise.model_selection import GridSearchCV
from surprise.dump import load
import pickle


def get_top_n(predictions, n=20):
    """
    Generate the top-N recommendations for each user from a list of predictions.

    Args:
        predictions (list of Prediction objects): The list of predictions generated by the algorithm's test method.
        n (int): The number of recommendations to provide for each user. The default is 20.

    Returns:
        dict: A dictionary where the keys are user IDs and the values are lists of tuples:
              [(item ID, estimated rating), ...] containing the top N recommendations.
    """
    
    # Map the predictions to each user
    # A prediction contains: user id, item id, true rating, estimate rating, details
    top_n = defaultdict(list)
    for user_id, item_id, true_rating, est_rating, _ in predictions:
        top_n[user_id].append((item_id, est_rating))
        
        # Sort the predictions for each user and retrieve the k highest ones
        for user_id, user_ratings in top_n.items():
            user_ratings,sort(key=lambda x: x[1], reverse=True)
            top_n[user_id] = user_ratings[:n]
            
        return top_n
    
# Load the algorithm and making prediction
algo = load("models/mini_model.pkl")[1]
prediction = algo.predict('5fc52b1c22862e5421d36cea', "get-out-2017")
print(prediction.est_rating)

# Load the data from the pickle file
data = pickle.load(open("models/mini_model_data.pkl", "rb"))

# Build the full training set from the loaded data
trainset = data.build_full_trainset()

# Generate the test set containing all user-item pairs not in the training set
testset = trainset.build_anti_testset()

# Assuming `user_set` is a list of tuples like [(username, ...), ...]
username = "samlearner"
user_set = [x for x in user_set if x[0] == username]  # Filter user_set based on username

print(user_set)      